<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Deep Koalarization: Manga Colorization Project</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            line-height: 1.6;
            color: #333;
            background-color: #f8f9fa;
        }

        .container {
            max-width: 1200px;
            margin: 0 auto;
            padding: 20px;
        }

        header {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 60px 20px;
            text-align: center;
            box-shadow: 0 4px 6px rgba(0,0,0,0.1);
        }

        header h1 {
            font-size: 2.5em;
            margin-bottom: 10px;
            font-weight: 700;
        }

        header p {
            font-size: 1.2em;
            opacity: 0.95;
            margin-bottom: 20px;
        }

        .authors {
            font-size: 1em;
            margin-top: 15px;
            opacity: 0.9;
        }

        .section {
            background: white;
            margin: 30px 0;
            padding: 40px;
            border-radius: 8px;
            box-shadow: 0 2px 10px rgba(0,0,0,0.08);
        }

        h2 {
            color: #667eea;
            font-size: 2em;
            margin-bottom: 20px;
            border-bottom: 3px solid #667eea;
            padding-bottom: 10px;
        }

        h3 {
            color: #764ba2;
            font-size: 1.5em;
            margin-top: 30px;
            margin-bottom: 15px;
        }

        p {
            margin-bottom: 15px;
            text-align: justify;
        }

        .highlight-box {
            background: #f0f4ff;
            border-left: 4px solid #667eea;
            padding: 20px;
            margin: 20px 0;
            border-radius: 4px;
        }

        .grid-container {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(300px, 1fr));
            gap: 20px;
            margin: 30px 0;
        }

        .card {
            background: #f8f9fa;
            padding: 20px;
            border-radius: 8px;
            border: 1px solid #e0e0e0;
        }

        .card h4 {
            color: #667eea;
            margin-bottom: 10px;
        }

        .image-container {
            text-align: center;
            margin: 30px 0;
        }

        .image-container img {
            max-width: 100%;
            height: auto;
            border-radius: 8px;
            box-shadow: 0 4px 12px rgba(0,0,0,0.15);
            margin: 10px 0;
        }

        .image-caption {
            font-style: italic;
            color: #666;
            margin-top: 10px;
            font-size: 0.95em;
        }

        .results-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
            gap: 30px;
            margin: 30px 0;
        }

        table {
            width: 100%;
            border-collapse: collapse;
            margin: 20px 0;
            background: white;
        }

        th, td {
            padding: 12px;
            text-align: left;
            border: 1px solid #ddd;
        }

        th {
            background: #667eea;
            color: white;
            font-weight: 600;
        }

        tr:nth-child(even) {
            background: #f8f9fa;
        }

        .impact-list {
            list-style: none;
            padding: 0;
        }

        .impact-list li {
            padding: 15px;
            margin: 10px 0;
            background: #f0f4ff;
            border-radius: 6px;
            border-left: 4px solid #667eea;
        }

        .impact-list li:before {
            content: "âœ“ ";
            color: #667eea;
            font-weight: bold;
            margin-right: 10px;
        }

        footer {
            background: #2d3748;
            color: white;
            text-align: center;
            padding: 30px 20px;
            margin-top: 50px;
        }

        .github-link {
            display: inline-block;
            background: #667eea;
            color: white;
            padding: 12px 30px;
            text-decoration: none;
            border-radius: 25px;
            margin-top: 20px;
            transition: background 0.3s;
        }

        .github-link:hover {
            background: #764ba2;
        }

        @media (max-width: 768px) {
            header h1 {
                font-size: 1.8em;
            }

            .section {
                padding: 20px;
            }

            .grid-container,
            .results-grid {
                grid-template-columns: 1fr;
            }
        }
    </style>
</head>
<body>
    <header>
        <div class="container">
            <h1>Deep Koalarization</h1>
            <p>Image Colorization using CNNs and Inception-ResNet-v2 for Manga</p>
            <div class="authors">
                <strong>Authors:</strong> Anish Mathur, Maulesh Gandhi, Pavan Kondooru<br>
                <em>IIIT Hyderabad</em>
            </div>
        </div>
    </header>

    <div class="container">
        <!-- Abstract Section -->
        <div class="section">
            <h2>Abstract</h2>
            <p>
                This project presents our implementation and experimentation based on the paper "Deep Koalarization: Image Colorization using CNNs and Inception-ResNet-v2" with a specific focus on manga colorization. We replicate the original model's results, explore alternative architectures and hyperparameters, and train a custom autoencoder for feature extraction. Our work demonstrates the effectiveness of deep learning approaches for automatic manga colorization, achieving high-quality results with minimal color bleeding and accurate semantic understanding.
            </p>
        </div>

        <!-- Methodology Section -->
        <div class="section">
            <h2>Methodology</h2>
            
            <h3>Architecture Overview</h3>
            <p>
                Our model uses a sophisticated encoder-decoder architecture combined with pre-trained semantic feature extraction. Given the luminance (L) component of a grayscale image, the network predicts the chrominance components (a* and b*) in the Lab color space, which are then combined to produce the final colorized image.
            </p>

            <div class="image-container">
                <img src="model.jpg" alt="Model Architecture">
                <p class="image-caption">Figure 1: Overall model architecture combining encoder-decoder with Inception-ResNet-v2 feature extraction</p>
            </div>

            <div class="highlight-box">
                <h4>Key Components:</h4>
                <ul style="margin-left: 20px; margin-top: 10px;">
                    <li><strong>Feature Extractor:</strong> Inception-ResNet-v2 pre-trained on ImageNet extracts high-level semantic features</li>
                    <li><strong>Encoder Network:</strong> Processes luminance channel with convolutional layers to capture spatial information</li>
                    <li><strong>Fusion Layer:</strong> Combines encoder output with semantic features to distribute context across all spatial regions</li>
                    <li><strong>Decoder Network:</strong> Upsamples fused features to predict chrominance channels at full resolution</li>
                </ul>
            </div>

            <div class="image-container">
                <img src="layers.jpg" alt="Network Layers">
                <p class="image-caption">Figure 2: Detailed layer specifications - Left: Encoder network, Center: Fusion network, Right: Decoder network</p>
            </div>

            <h3>Training Process</h3>
            <p>
                We trained our models on two datasets: ImageNet (50,000 images) and Danbooru2020small (manga dataset). Images were preprocessed by converting to Lab color space and normalizing. The training utilized ADA GPUs with a batch size of 32, with most models trained for 20-100 epochs depending on the experiment. We split data into 80% training, 10% validation, and 10% testing sets.
            </p>
        </div>

        <!-- Experiments Section -->
        <div class="section">
            <h2>Experiments & Results</h2>

            <h3>1. Learning Rate Analysis</h3>
            <p>
                We conducted extensive experiments with different learning rates (0.00001, 0.0001, 0.0005, 0.01) to understand their impact on convergence and colorization quality. Our findings show that 0.0005 provided the best balance between training speed and result quality.
            </p>

            <div class="image-container">
                <img src="vals.jpg" alt="Learning Rate Comparison">
                <p class="image-caption">Figure 3: Test loss and accuracy across different learning rates and epochs</p>
            </div>

            <table>
                <thead>
                    <tr>
                        <th>Learning Rate</th>
                        <th>Epochs</th>
                        <th>Test Loss</th>
                        <th>Test Accuracy</th>
                        <th>Dataset</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td>0.00001</td>
                        <td>20</td>
                        <td>0.0116</td>
                        <td>0.6594</td>
                        <td>ImageNet</td>
                    </tr>
                    <tr>
                        <td>0.0001</td>
                        <td>50</td>
                        <td>0.0128</td>
                        <td>0.6489</td>
                        <td>ImageNet</td>
                    </tr>
                    <tr>
                        <td>0.0005</td>
                        <td>100</td>
                        <td>0.0080</td>
                        <td>0.7478</td>
                        <td>Manga</td>
                    </tr>
                    <tr>
                        <td>0.01</td>
                        <td>20</td>
                        <td>0.9655</td>
                        <td>0.6327</td>
                        <td>ImageNet</td>
                    </tr>
                </tbody>
            </table>

            <div class="results-grid">
                <div class="image-container">
                    <img src="C_bird.jpg" alt="Bird colorization with different learning rates">
                    <p class="image-caption">Learning rate comparison on bird image</p>
                </div>
                <div class="image-container">
                    <img src="C_chair.jpg" alt="Chair colorization with different learning rates">
                    <p class="image-caption">Learning rate comparison on furniture</p>
                </div>
                <div class="image-container">
                    <img src="C_dog.jpg" alt="Dog colorization with different learning rates">
                    <p class="image-caption">Learning rate comparison on animal</p>
                </div>
                <div class="image-container">
                    <img src="C_Man.jpg" alt="Portrait colorization with different learning rates">
                    <p class="image-caption">Learning rate comparison on portrait</p>
                </div>
            </div>

            <h3>2. Feature Extractor Comparison</h3>
            <p>
                We compared Inception-ResNet-v2 with Inception-v3 as the feature extractor. Inception-ResNet-v2 consistently outperformed Inception-v3 due to its residual connections and batch normalization, which help prevent vanishing gradients and enable better semantic understanding.
            </p>

            <div class="results-grid">
                <div class="image-container">
                    <img src="B_bird.jpg" alt="Bird colorization comparison">
                    <p class="image-caption">Inception-v3 vs Inception-ResNet-v2 on birds</p>
                </div>
                <div class="image-container">
                    <img src="B_chair.jpg" alt="Chair colorization comparison">
                    <p class="image-caption">Different feature extractors on furniture</p>
                </div>
            </div>

            <div class="image-container">
                <img src="B_misc.jpg" alt="Complex scene colorization">
                <p class="image-caption">Figure 4: Feature extractor comparison on complex outdoor scenes</p>
            </div>

            <h3>3. Custom Autoencoder</h3>
            <p>
                We trained a custom autoencoder from scratch on 30,000 ImageNet images to replace the pre-trained Inception-ResNet-v2 feature extractor. This experiment explored whether domain-specific feature learning could improve results.
            </p>

            <div class="image-container">
                <img src="autoencoder.jpg" alt="Autoencoder Architecture">
                <p class="image-caption">Figure 5: Custom autoencoder architecture for feature extraction</p>
            </div>

            <div class="image-container">
                <img src="auto.jpg" alt="Autoencoder Results">
                <p class="image-caption">Figure 6: Colorization results using custom autoencoder features</p>
            </div>

            <h3>4. Manga Colorization Results</h3>
            <p>
                Our primary contribution is the successful application of Deep Koalarization to manga images. The model achieves exceptional results on manga artwork, with accurate color assignment, minimal bleeding, and strong preservation of line art. The simpler visual structure of manga (compared to photographs) allows the model to produce near-ground-truth quality colorizations.
            </p>

            <div class="results-grid">
                <div class="image-container">
                    <img src="D_Man.jpg" alt="Manga male character colorization">
                    <p class="image-caption">Male character colorization comparison</p>
                </div>
                <div class="image-container">
                    <img src="D_woman.jpg" alt="Manga female character colorization">
                    <p class="image-caption">Female character colorization comparison</p>
                </div>
                <div class="image-container">
                    <img src="D_unkown.jpg" alt="Manga character colorization">
                    <p class="image-caption">Complex character with multiple colors</p>
                </div>
            </div>
        </div>

        <!-- Impact Section -->
        <div class="section">
            <h2>Project Impact</h2>

            <div class="grid-container">
                <div class="card">
                    <h4>ðŸŽ¨ Creative Industry Applications</h4>
                    <p>Automated manga colorization can significantly reduce production time and costs for publishers and independent artists, making colored manga more accessible to global audiences.</p>
                </div>

                <div class="card">
                    <h4>ðŸ“š Historical Preservation</h4>
                    <p>The technique can be applied to restore and colorize historical photographs and archival materials, bringing history to life for modern viewers and educational purposes.</p>
                </div>

                <div class="card">
                    <h4>ðŸ”¬ Research Contributions</h4>
                    <p>Our work demonstrates that Deep Koalarization is highly effective for stylized artwork, opening new research directions for domain-specific colorization approaches.</p>
                </div>
            </div>

            <h3>Key Achievements</h3>
            <ul class="impact-list">
                <li>Successfully replicated Deep Koalarization paper results with limited computational resources</li>
                <li>Achieved 74.78% test accuracy on manga dataset with optimized hyperparameters</li>
                <li>Demonstrated superior performance of Inception-ResNet-v2 over Inception-v3 for feature extraction</li>
                <li>Identified optimal learning rate (0.0005) for this colorization task through systematic experimentation</li>
                <li>Proved feasibility of custom autoencoder feature extractors for specialized domains</li>
                <li>Created high-quality colorization results comparable to ground truth on manga artwork</li>
            </ul>

            <div class="highlight-box">
                <h4>Technical Innovation:</h4>
                <p>
                    Our implementation showcases the power of combining pre-trained semantic understanding with task-specific encoder-decoder architectures. The fusion mechanism effectively distributes high-level features across spatial dimensions, enabling the model to make context-aware colorization decisions. This approach is particularly effective for structured content like manga, where semantic understanding of characters, clothing, and scene elements is crucial.
                </p>
            </div>
        </div>

        <!-- Conclusion Section -->
        <div class="section">
            <h2>Conclusion</h2>
            <p>
                This project successfully demonstrates the application of Deep Koalarization to manga colorization, achieving impressive results that rival manual colorization. Through systematic experimentation with learning rates, feature extractors, and custom architectures, we gained valuable insights into the model's behavior and optimal configurations.
            </p>
            <p>
                The success on manga images suggests that this approach is particularly well-suited for stylized artwork with clear boundaries and distinct semantic regions. Future work could explore video colorization, real-time processing, and user-guided colorization with color hints.
            </p>
            <p>
                Our findings contribute to both the academic understanding of deep learning-based colorization and practical applications in the creative industry, demonstrating that state-of-the-art computer vision techniques can be effectively adapted for artistic and commercial purposes.
            </p>
        </div>

        <!-- Reference Section -->
        <div class="section">
            <h2>Reference</h2>
            <p style="text-align: left; margin-left: 20px;">
                Baldassarre, F., GonzÃ¡lez MorÃ­n, D., & RodÃ©s-Guirao, L. (2017). <em>Deep Koalarization: Image Colorization using CNNs and Inception-ResNet-v2.</em> arXiv preprint arXiv:1712.03400.
            </p>
        </div>
    </div>

    <footer>
        <div class="container">
            <p>&copy; 2025 Maulesh Gandhi, Anish Mathur, Pavan Kondooru | IIIT Hyderabad</p>
            <p style="margin-top: 10px; font-size: 0.9em;">Deep Koalarization: Manga Colorization Project</p>
        </div>
    </footer>
</body>
</html>
